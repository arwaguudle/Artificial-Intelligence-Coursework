{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1.lOADING THE DATA ---\n",
    "#Adjust the paths to match where you've unzipped \"UCI HAR Dataset\"\n",
    "\n",
    "PATH = \"/Users/ahmed/Desktop/Artificial-Intelligence-Coursework/UCI HAR Dataset\"\n",
    "\n",
    "features_path = PATH + \"/features.txt\"\n",
    "activity_labels_path = PATH + \"/activity_labels.txt\"\n",
    "X_train_path = PATH + \"/train/X_train.txt\"\n",
    "y_train_path = PATH + \"/train/y_train.txt\"\n",
    "X_test_path = PATH + \"/test/X_test.txt\"\n",
    "y_test_path = PATH + \"/test/y_test.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature names, This appends the column index to any duplicate names.\n",
    "\n",
    "features_df = pd.read_csv(features_path, sep=\"\\s+\", header=None, names=[\"idx\", \"feature\"])\n",
    "feature_names = features_df[\"feature\"].tolist()\n",
    "\n",
    "\n",
    "# his appends the column index to any duplicate names.\n",
    "features_df[\"feature\"] = features_df[\"feature\"].astype(str) + \"_\" + features_df.index.astype(str)\n",
    "feature_names = features_df[\"feature\"].tolist()\n",
    "\n",
    "\n",
    "# Load activity labels (mapping IDs 1-6 to string names)\n",
    "activity_labels_df = pd.read_csv(activity_labels_path, sep=\"\\s+\", header=None, names=[\"id\", \"activity\"])\n",
    "activity_map = dict(zip(activity_labels_df[\"id\"], activity_labels_df[\"activity\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the training data\n",
    "X_train = pd.read_csv(X_train_path, sep=\"\\s+\", header=None, names=feature_names)\n",
    "y_train = pd.read_csv(y_train_path, sep=\"\\s+\", header=None, names=[\"activity\"])\n",
    "X_test = pd.read_csv(X_test_path, sep=\"\\s+\", header=None, names=feature_names)\n",
    "y_test = pd.read_csv(y_test_path, sep=\"\\s+\", header=None, names=[\"activity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map activity IDs to their names\n",
    "y_train[\"activity\"] = y_train[\"activity\"].map(activity_map)\n",
    "y_test[\"activity\"] = y_test[\"activity\"].map(activity_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    " # ---2.CONVERT MULTI-CLASS LABELS TO BINARY ---\n",
    "def to_binary_label(activity):\n",
    "    if activity in [\"WALKING\", \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\"]:\n",
    "        return 1 #Active\n",
    "    else:\n",
    "        return 0 #Inactive\n",
    "    \n",
    "y_train_binary = y_train[\"activity\"].apply(to_binary_label)\n",
    "y_test_binary = y_test[\"activity\"].apply(to_binary_label)\n",
    "\n",
    "\n",
    "#y_train_binary = y_train_binary.astype(int).values.ravel()\n",
    "#y_test_binary = y_test_binary.astype(int).values.ravel()\n",
    "#Now we have 0/1 labels in y_train[\"Binary\"] and y_test[\"Binary\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduction of the number of features\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=50)), #reduce from 561 -> 50 \n",
    "    (\"svm\", SVC(class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "#can tweak n components depending on how much dimension reduction you want vs how much#\n",
    "#computational time you can spare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 1.00\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "pipeline.fit(X_train, y_train_binary)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test_binary, y_pred)\n",
    "precision = precision_score(y_test_binary, y_pred)\n",
    "recall = recall_score(y_test_binary, y_pred)\n",
    "\n",
    "print(\"F1 Score: {:.2f}\".format(f1))\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the baseline svm models with different kernels (linear, polynomial, rbf)\n",
    "param_grid = [\n",
    "    {\n",
    "        \"svm__kernel\": [\"linear\"],\n",
    "        \"svm__C\": [0.1, 1, 10]\n",
    "    },\n",
    "    {\n",
    "        \"svm__kernel\": [\"poly\"],\n",
    "        \"svm__degree\": [2, 3, 4],\n",
    "        \"svm__gamma\": [0.1, 1],\n",
    "        \"svm__C\": [0.1, 1, 10]\n",
    "    },\n",
    "    {\n",
    "        \"svm__kernel\": [\"rbf\"],\n",
    "        \"svm__C\": [0.1, 1, 10],\n",
    "        \"svm__gamma\": [0.1, 1]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 27 candidates, totalling 54 fits\n",
      "Best parameters:  {'svm__C': 0.1, 'svm__degree': 3, 'svm__gamma': 1, 'svm__kernel': 'poly'}\n",
      "Best score:  0.999183895538629\n"
     ]
    }
   ],
   "source": [
    "#grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=2,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train_binary)\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Best parameters:  {'svm__kernel': 'poly', 'svm__gamma': 0.1, 'svm__degree': 3, 'svm__C': 1}\n",
      "Best score:  0.999183895538629\n"
     ]
    }
   ],
   "source": [
    "#it took too long, so we are going do a RandomizedSearchCV instead\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=2,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train_binary)\n",
    "print(\"Best parameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1560\n",
      "           1       1.00      1.00      1.00      1387\n",
      "\n",
      "    accuracy                           1.00      2947\n",
      "   macro avg       1.00      1.00      1.00      2947\n",
      "weighted avg       1.00      1.00      1.00      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.metrics import classification_report\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "y_pred_binary = y_pred\n",
    "\n",
    "print(classification_report(y_test_binary, y_pred_binary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
